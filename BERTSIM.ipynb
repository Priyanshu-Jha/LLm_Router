{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a5225c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2162bda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "preprocessed_data_path = 'preprocessed_dataset.csv'  # Path to preprocessed dataset\n",
    "output_data_path = 'dataset_with_bertsim_softlabels.csv'  # Path to save updated dataset\n",
    "SENTENCE_TRANSFORMER_MODEL = 'all-MiniLM-L6-v2'  # Sentence transformer model\n",
    "TEMPERATURE = 10.0  # Temperature for softmax scaling\n",
    "BATCH_SIZE = 32  # Batch size for processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc94be00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define response columns (reduced to 3 models)\n",
    "response_columns = [\n",
    "    'gpt-3.5-turbo-1106|model_response',\n",
    "    'claude-instant-v1|model_response',\n",
    "    'claude-v1|model_response',\n",
    "    'claude-v2|model_response',\n",
    "    'meta/llama-2-70b-chat|model_response',\n",
    "    'mistralai/mixtral-8x7b-chat|model_response',\n",
    "    'zero-one-ai/Yi-34B-Chat|model_response',\n",
    "    'WizardLM/WizardLM-13B-V1.2|model_response',\n",
    "    'meta/code-llama-instruct-34b-chat|model_response',\n",
    "    'mistralai/mistral-7b-chat|model_response'\n",
    "\n",
    "]\n",
    "ground_truth_column = 'gpt-4-1106-preview|model_response'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f175a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed dataset\n",
    "df = pd.read_csv(preprocessed_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7c46432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Set device (GPU if available, else CPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2dd3819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize sentence transformer\n",
    "sentence_model = SentenceTransformer(SENTENCE_TRANSFORMER_MODEL)\n",
    "sentence_model = sentence_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a2e2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_bertsim_scores(ground_truths, model_responses, sentence_model, batch_size, device):\n",
    "    bertsim_scores = []\n",
    "    num_models = len(model_responses[0])  # Number of models\n",
    "\n",
    "    # Process ground truths in batches\n",
    "    valid_ground_truths = [gt for gt in ground_truths if gt and not pd.isna(gt)]\n",
    "    gt_indices = [i for i, gt in enumerate(ground_truths) if gt and not pd.isna(gt)]\n",
    "    gt_embeddings = []\n",
    "\n",
    "    for i in range(0, len(valid_ground_truths), batch_size):\n",
    "        batch = valid_ground_truths[i:i + batch_size]\n",
    "        embeddings = sentence_model.encode(batch, convert_to_tensor=True, device=device, show_progress_bar=False)\n",
    "        gt_embeddings.extend(embeddings)\n",
    "    \n",
    "    # Map embeddings back to original indices\n",
    "    gt_embedding_dict = {idx: emb for idx, emb in zip(gt_indices, gt_embeddings)}\n",
    "\n",
    "    # Process model responses for each model\n",
    "    for model_idx in range(num_models):\n",
    "        model_scores = []\n",
    "        # Extract responses for this model across all rows\n",
    "        responses = [row[model_idx] for row in model_responses]\n",
    "        valid_responses = [resp for resp in responses if resp and not pd.isna(resp)]\n",
    "        resp_indices = [i for i, resp in enumerate(responses) if resp and not pd.isna(resp)]\n",
    "        resp_embeddings = []\n",
    "\n",
    "        # Encode responses in batches\n",
    "        for i in range(0, len(valid_responses), batch_size):\n",
    "            batch = valid_responses[i:i + batch_size]\n",
    "            embeddings = sentence_model.encode(batch, convert_to_tensor=True, device=device, show_progress_bar=False)\n",
    "            resp_embeddings.extend(embeddings)\n",
    "\n",
    "        # Map embeddings back to original indices\n",
    "        resp_embedding_dict = {idx: emb for idx, emb in zip(resp_indices, resp_embeddings)}\n",
    "\n",
    "        # Compute scores for each row\n",
    "        for row_idx in range(len(ground_truths)):\n",
    "            if row_idx not in gt_embedding_dict or row_idx not in resp_embedding_dict:\n",
    "                score = 0.0\n",
    "            else:\n",
    "                gt_emb = gt_embedding_dict[row_idx]\n",
    "                resp_emb = resp_embedding_dict[row_idx]\n",
    "                score = util.cos_sim(gt_emb, resp_emb).item()\n",
    "            model_scores.append(score)\n",
    "        bertsim_scores.append(model_scores)\n",
    "\n",
    "    return np.array(bertsim_scores).T  # Transpose to have shape (num_rows, num_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f49877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate soft labels\n",
    "def generate_soft_labels(bertsim_scores, temperature):\n",
    "    scaled_scores = bertsim_scores / temperature\n",
    "    exp_scores = np.exp(scaled_scores)\n",
    "    # Avoid division by zero by adding a small epsilon\n",
    "    sum_exp_scores = np.sum(exp_scores, axis=1, keepdims=True)\n",
    "    sum_exp_scores = np.where(sum_exp_scores == 0, 1e-10, sum_exp_scores)\n",
    "    soft_labels = exp_scores / sum_exp_scores\n",
    "    return soft_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6159134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "ground_truths = df[ground_truth_column].tolist()\n",
    "model_responses = df[response_columns].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60dbe52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute BERTSim scores\n",
    "bertsim_scores = compute_bertsim_scores(ground_truths, model_responses, sentence_model, BATCH_SIZE, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fc95a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate soft labels\n",
    "soft_labels = generate_soft_labels(bertsim_scores, TEMPERATURE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49c3f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add BERTSim scores and soft labels to dataframe\n",
    "model_names = [col.split('|')[0] for col in response_columns]\n",
    "for i, model_name in enumerate(model_names):\n",
    "    df[f'{model_name}|BERTSim'] = bertsim_scores[:, i]\n",
    "    df[f'{model_name}|soft_label'] = soft_labels[:, i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1667935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save updated dataset\n",
    "df.to_csv(output_data_path, index=False)\n",
    "\n",
    "print(f\"BERTSim scores and soft labels computed. Dataset saved as '{output_data_path}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8ebe6a95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New deduplicated dataset has 8725 rows and is saved as dataset_with_bertsim_softlabels_deduped.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"dataset_with_bertsim_softlabels.csv\")\n",
    "\n",
    "# Remove duplicate rows based on the 'prompts' column and create a new DataFrame\n",
    "deduped_df = df.drop_duplicates(subset=['prompt'], keep='first').reset_index(drop=True)\n",
    "\n",
    "# Optionally, save the new deduplicated dataset to a file\n",
    "deduped_df.to_csv(\"dataset_with_bertsim_softlabels_deduped.csv\", index=False)\n",
    "\n",
    "print(f\"New deduplicated dataset has {len(deduped_df)} rows and is saved as dataset_with_bertsim_softlabels_deduped.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "49c7ab8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered data saved to filtered_bertsim_softlabels.csv and filtered_bertsim_softlabels.json\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"dataset_with_bertsim_softlabels_deduped.csv\")\n",
    "\n",
    "# Columns to keep\n",
    "cols_to_keep = [\n",
    "    \"prompt\",\n",
    "    \"oracle_model_to_route_to\",\n",
    "    \"GroundTruth\",\n",
    "    \"mistralai/mixtral-8x7b-chat|BERTSim\",\n",
    "    \"mistralai/mixtral-8x7b-chat|soft_label\",\n",
    "    \"zero-one-ai/Yi-34B-Chat|BERTSim\",\n",
    "    \"zero-one-ai/Yi-34B-Chat|soft_label\",\n",
    "    \"mistralai/mistral-7b-chat|BERTSim\",\n",
    "    \"mistralai/mistral-7b-chat|soft_label\",\n",
    "    \"mistralai/mixtral-8x7b-chat|model_response\",\n",
    "    \"mistralai/mistral-7b-chat|model_response\",\n",
    "    \"zero-one-ai/Yi-34B-Chat|model_response\"\n",
    "]\n",
    "\n",
    "# Filter the DataFrame\n",
    "filtered_df = df[cols_to_keep]\n",
    "\n",
    "# Save to CSV\n",
    "filtered_df.to_csv(\"Dataset_JSON/filtered_bertsim_softlabels.csv\", index=False)\n",
    "\n",
    "# Save to JSON\n",
    "filtered_df.to_json(\"Dataset_JSON/filtered_bertsim_softlabels.json\", orient=\"records\", indent=2, force_ascii=False)\n",
    "\n",
    "print(\"Filtered data saved to filtered_bertsim_softlabels.csv and filtered_bertsim_softlabels.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495e217f",
   "metadata": {},
   "source": [
    "### Calculating cost for each prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9908c7a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost columns added and saved to Dataset_JSON/filtered_bertsim_softlabels_with_cost.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load your JSON\n",
    "with open('Dataset_JSON/filtered_bertsim_softlabels.json', 'r', encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Define model info: field name and cost per token\n",
    "model_info = {\n",
    "    \"mistralai/mixtral-8x7b-chat\": {\n",
    "        \"field\": \"mistralai/mixtral-8x7b-chat|model_response\",\n",
    "        \"cost_per_token\": 0.60  # per 1M tokens\n",
    "    },\n",
    "    \"mistralai/mistral-7b-chat\": {\n",
    "        \"field\": \"mistralai/mistral-7b-chat|model_response\",\n",
    "        \"cost_per_token\": 0.2   # per 1M tokens\n",
    "    },\n",
    "    \"zero-one-ai/Yi-34B-Chat\": {\n",
    "        \"field\": \"zero-one-ai/Yi-34B-Chat|model_response\",\n",
    "        \"cost_per_token\": 0.8   # per 1M tokens\n",
    "    }\n",
    "}\n",
    "\n",
    "# Simple whitespace tokenizer\n",
    "def count_tokens(text):\n",
    "    if not isinstance(text, str):\n",
    "        return 0\n",
    "    return len(text.split())\n",
    "\n",
    "# Process each row\n",
    "for row in data:\n",
    "    prompt = row.get(\"prompt\", \"\")\n",
    "    for model, info in model_info.items():\n",
    "        # Count input tokens\n",
    "        input_tokens = count_tokens(prompt)\n",
    "        # Count output tokens\n",
    "        output_text = row.get(info[\"field\"], \"\")\n",
    "        output_tokens = count_tokens(output_text)\n",
    "        # Calculate cost\n",
    "        cost = (input_tokens * info[\"cost_per_token\"] / 1e6) + (output_tokens * info[\"cost_per_token\"] / 1e6)\n",
    "        # Add new column\n",
    "        cost_field = info[\"field\"].replace(\"|model_response\", \"|cost\")\n",
    "        row[cost_field] = cost\n",
    "\n",
    "# Save to new JSON\n",
    "with open('Dataset_JSON/filtered_bertsim_softlabels_with_cost.json', 'w', encoding=\"utf-8\") as f:\n",
    "    json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"Cost columns added and saved to Dataset_JSON/filtered_bertsim_softlabels_with_cost.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb59d267",
   "metadata": {},
   "source": [
    "### Calculating Throughput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1f1d1b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8725/8725 [00:00<00:00, 47217.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! Throughput columns added.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "# You may want to use a real tokenizer for accurate token counts.\n",
    "def count_tokens(text):\n",
    "    # Replace with your tokenizer if needed\n",
    "    return len(text.split())\n",
    "\n",
    "# Model parameters\n",
    "model_params = {\n",
    "    \"mistralai/mistral-7b-chat|model_response\": {\n",
    "        \"avg_time_to_first_token\": 0.18,\n",
    "        \"avg_decode_rate\": 175\n",
    "    },\n",
    "    \"zero-one-ai/Yi-34B-Chat|model_response\": {\n",
    "        \"avg_time_to_first_token\": 0.25,\n",
    "        \"avg_decode_rate\": 108\n",
    "    },\n",
    "    \"mistralai/mixtral-8x7b-chat|model_response\": {\n",
    "        \"avg_time_to_first_token\": 0.35,\n",
    "        \"avg_decode_rate\": 54\n",
    "    }\n",
    "}\n",
    "\n",
    "# Load your JSON file\n",
    "with open('Dataset_JSON/filtered_bertsim_softlabels_with_cost.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "for item in tqdm(data):\n",
    "    prompt = item['prompt']\n",
    "    input_tokens = count_tokens(prompt)\n",
    "    for model_col, params in model_params.items():\n",
    "        response = item.get(model_col, \"\")\n",
    "        output_tokens = count_tokens(response)\n",
    "        avg_time_to_first_token = params[\"avg_time_to_first_token\"]\n",
    "        avg_decode_rate = params[\"avg_decode_rate\"]\n",
    "        # Time from query submission to completion\n",
    "        total_time = avg_time_to_first_token + (output_tokens / avg_decode_rate)\n",
    "        # Throughput calculation\n",
    "        throughput = output_tokens / total_time if total_time > 0 else 0\n",
    "        # Add new column\n",
    "        throughput_col = model_col.replace('|model_response', '|throughput')\n",
    "        item[throughput_col] = throughput\n",
    "\n",
    "# Save back to JSON\n",
    "with open('Dataset_JSON/final.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"Done! Throughput columns added.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7d1314",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('Dataset_JSON/final.csv')\n",
    "\n",
    "# List of columns to remove\n",
    "columns_to_remove = [\n",
    "    'mistralai/mistral-7b-chat|soft_label',\n",
    "    'zero-one-ai/Yi-34B-Chat|soft_label',\n",
    "    'mistralai/mixtral-8x7b-chat|soft_label'\n",
    "]\n",
    "\n",
    "# Remove the columns (ignore errors if columns are missing)\n",
    "df = df.drop(columns=columns_to_remove, errors='ignore')\n",
    "\n",
    "# Save to new CSV\n",
    "df.to_csv('final1.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
